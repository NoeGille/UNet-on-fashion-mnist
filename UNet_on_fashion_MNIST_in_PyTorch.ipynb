{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNjoYvhc4JJHUiE1+5iqKVP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NoeGille/UNet-on-fashion-mnist/blob/main/UNet_on_fashion_MNIST_in_PyTorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "mpYPHZCfGCMa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "L9SuWZHWFOzL"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "in_channels = 1\n",
        "batch_size = 64\n",
        "learning_rate = 0.001\n",
        "num_classes = 10\n",
        "epochs = 1\n",
        "\n",
        "\n",
        "train_dataset = datasets.FashionMNIST(root='tmp/dataset/', train=True, transform=transforms.ToTensor(), download=True)\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_dataset = datasets.FashionMNIST(root='tmp/dataset/', train=False, transform=transforms.ToTensor(), download=True)\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=True)\n"
      ],
      "metadata": {
        "id": "qoQIMSljGGSY"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DownSampleBlock(nn.Module):\n",
        "    '''Reduce the dimension of the image in input by 2'''\n",
        "    def __init__(self, in_channels):\n",
        "        super().__init__()\n",
        "        # We keep the same dimension in input and ouput\n",
        "        self.conv1 = nn.Conv2d(in_channels=in_channels, out_channels=in_channels, kernel_size=(3, 3), stride=(1, 1), padding=(1,1))\n",
        "        self.relu = nn.ReLu()\n",
        "        self.conv2 = nn.Conv2d(in_channels=in_channels, out_channels=in_channels, kernel_size=(3, 3), stride=(1, 1), padding=(1,1))\n",
        "        self.pool = nn.MaxPool2d(kernel_size=(2, 2), stride=(2,2))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(self.conv2(self.relu(self.conv1(x))))\n",
        "        return x\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, input_size, num_classes:int=10, depth:int=2):\n",
        "        '''### Initialize a UNet model\n",
        "        input_size : dimension of input\n",
        "        num_classes : specify the number of classes in ouput\n",
        "        depth : the number of blocks (depth of the model)'''\n",
        "        super().__init__()\n",
        "        # Reduce size of inputs\n",
        "        dblocks = []\n",
        "        self.bottleneck = Bottleneck()\n",
        "        # Increase size of inputs\n",
        "        ublocks = []\n",
        "        for i in range(depth):\n",
        "            dblocks.append(DownSampleBlock())\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9a5UTxiUKW4L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def transform_targets(X, y_label):\n",
        "\n",
        "    y = np.array([cv2.threshold(img, 8, 255, type=cv2.THRESH_BINARY)[1] for img in X.numpy()])\n",
        "    y = np.where(y == 0, 0, 1)\n",
        "    for s in range(y.shape[0]):\n",
        "      y[s] *= y_label[s].numpy() + 1\n",
        "    return torch.from_numpy(np.eye(num_classes, dtype='uint8')[y])\n",
        "\n",
        "\n",
        "for data,y_label in tqdm(train_loader):\n",
        "  targets = transform_targets(data, y_label)\n",
        "\n",
        "  data.to(device=device)\n",
        "  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K8sZpi3zGt71",
        "outputId": "6d60ebc5-1729-463b-fa4a-2f2114805fe9"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 938/938 [00:12<00:00, 72.96it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iHXA5TcyHbPs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}